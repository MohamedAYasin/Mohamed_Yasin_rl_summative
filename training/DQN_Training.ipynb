{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha4n4AigK9FA",
        "outputId": "cb98cf54-9976-4343-f73b-5c76ee5da332"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the directory where custom_env.py is located\n",
        "sys.path.append('/content/')\n"
      ],
      "metadata": {
        "id": "9Du5YOprJ0iu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrBT8vmDJ11X",
        "outputId": "4b0bbc2d-8aa3-4d73-b80e-11921a493ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15616    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 93732    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.25e-06 |\n",
            "|    n_updates        | 23407    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15620    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 93756    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.05e-06 |\n",
            "|    n_updates        | 23413    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15624    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 93780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.08e-06 |\n",
            "|    n_updates        | 23419    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15628    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 93804    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.6e-06  |\n",
            "|    n_updates        | 23425    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15632    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93828    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.48e-06 |\n",
            "|    n_updates        | 23431    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15636    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93852    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.63e-06 |\n",
            "|    n_updates        | 23437    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15640    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93876    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.27e-07 |\n",
            "|    n_updates        | 23443    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15644    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93900    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.97e-06 |\n",
            "|    n_updates        | 23449    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15648    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93924    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.67e-07 |\n",
            "|    n_updates        | 23455    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15652    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93948    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.32e-07 |\n",
            "|    n_updates        | 23461    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15656    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93972    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.96e-07 |\n",
            "|    n_updates        | 23467    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15660    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 93996    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.64e-07 |\n",
            "|    n_updates        | 23473    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15664    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94020    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.19e-07 |\n",
            "|    n_updates        | 23479    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15668    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94044    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.15e-06 |\n",
            "|    n_updates        | 23485    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15672    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94068    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45e-06 |\n",
            "|    n_updates        | 23491    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15676    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94092    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13e-06 |\n",
            "|    n_updates        | 23497    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15680    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94116    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.37e-06 |\n",
            "|    n_updates        | 23503    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15684    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94140    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36e-05 |\n",
            "|    n_updates        | 23509    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15688    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94164    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.34e-05 |\n",
            "|    n_updates        | 23515    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15692    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94188    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.83e-06 |\n",
            "|    n_updates        | 23521    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15696    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94212    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.9e-06  |\n",
            "|    n_updates        | 23527    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15700    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94236    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.4e-06  |\n",
            "|    n_updates        | 23533    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15704    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94260    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.15e-06 |\n",
            "|    n_updates        | 23539    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15708    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94284    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23e-06 |\n",
            "|    n_updates        | 23545    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15712    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94308    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.92e-06 |\n",
            "|    n_updates        | 23551    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.35     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15716    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94332    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.41e-07 |\n",
            "|    n_updates        | 23557    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.34     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15720    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94356    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.53e-06 |\n",
            "|    n_updates        | 23563    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.34     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15724    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94380    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34e-06 |\n",
            "|    n_updates        | 23569    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.34     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15728    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94404    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.3e-06  |\n",
            "|    n_updates        | 23575    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.34     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15732    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 94428    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.18e-06 |\n",
            "|    n_updates        | 23581    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.34     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15736    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94452    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.37e-05 |\n",
            "|    n_updates        | 23587    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.33     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15740    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94476    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.052    |\n",
            "|    n_updates        | 23593    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15744    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.7e-05  |\n",
            "|    n_updates        | 23599    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15748    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94524    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.92e-05 |\n",
            "|    n_updates        | 23605    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15752    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94548    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06e-05 |\n",
            "|    n_updates        | 23611    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15756    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94572    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11e-05 |\n",
            "|    n_updates        | 23617    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15760    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94596    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.17e-06 |\n",
            "|    n_updates        | 23623    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15764    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94620    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.66e-06 |\n",
            "|    n_updates        | 23629    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15768    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94644    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2e-06    |\n",
            "|    n_updates        | 23635    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15772    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.83e-06 |\n",
            "|    n_updates        | 23641    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15776    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94692    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.03e-06 |\n",
            "|    n_updates        | 23647    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15780    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94716    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.28e-07 |\n",
            "|    n_updates        | 23653    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15784    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94740    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.22e-06 |\n",
            "|    n_updates        | 23659    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15788    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94764    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.8e-06  |\n",
            "|    n_updates        | 23665    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15792    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94788    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.41e-05 |\n",
            "|    n_updates        | 23671    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15796    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94812    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23e-05 |\n",
            "|    n_updates        | 23677    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15800    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94836    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.53e-05 |\n",
            "|    n_updates        | 23683    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15804    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94860    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.56e-05 |\n",
            "|    n_updates        | 23689    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15808    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94884    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.61e-05 |\n",
            "|    n_updates        | 23695    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15812    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94908    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34e-05 |\n",
            "|    n_updates        | 23701    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15816    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 94932    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.88e-06 |\n",
            "|    n_updates        | 23707    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15820    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 94956    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.89e-06 |\n",
            "|    n_updates        | 23713    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15824    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 94980    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.61e-06 |\n",
            "|    n_updates        | 23719    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Eval num_timesteps=95000, episode_reward=1.40 +/- 0.00\n",
            "Episode length: 6.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 6        |\n",
            "|    mean_reward      | 1.4      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 95000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.68e-06 |\n",
            "|    n_updates        | 23724    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15828    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95006    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.92e-06 |\n",
            "|    n_updates        | 23726    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15832    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95030    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.34e-07 |\n",
            "|    n_updates        | 23732    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15836    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95054    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.68e-07 |\n",
            "|    n_updates        | 23738    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15840    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95078    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.67e-06 |\n",
            "|    n_updates        | 23744    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15844    |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95102    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.4e-07  |\n",
            "|    n_updates        | 23750    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15848    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95126    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.12e-07 |\n",
            "|    n_updates        | 23756    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15852    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95150    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.67e-07 |\n",
            "|    n_updates        | 23762    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15856    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95174    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09e-06 |\n",
            "|    n_updates        | 23768    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15860    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95198    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61e-06 |\n",
            "|    n_updates        | 23774    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15864    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95222    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.32e-07 |\n",
            "|    n_updates        | 23780    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15868    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95246    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02e-06 |\n",
            "|    n_updates        | 23786    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15872    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95270    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.04e-06 |\n",
            "|    n_updates        | 23792    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15876    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95294    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76e-06 |\n",
            "|    n_updates        | 23798    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15880    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95318    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.8e-07  |\n",
            "|    n_updates        | 23804    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.14     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15884    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95342    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46e-06 |\n",
            "|    n_updates        | 23810    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.14     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15888    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95366    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.63e-07 |\n",
            "|    n_updates        | 23816    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.11     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15892    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95390    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.19e-06 |\n",
            "|    n_updates        | 23822    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.13     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15896    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95414    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.12e-06 |\n",
            "|    n_updates        | 23828    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15900    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 95438    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.04e-07 |\n",
            "|    n_updates        | 23834    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.09     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15904    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95462    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07e-06 |\n",
            "|    n_updates        | 23840    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15908    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95486    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74e-06 |\n",
            "|    n_updates        | 23846    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15912    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95510    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.35e-07 |\n",
            "|    n_updates        | 23852    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15916    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95534    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.13e-06 |\n",
            "|    n_updates        | 23858    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15920    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95558    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16e-06 |\n",
            "|    n_updates        | 23864    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15924    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95582    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.81e-07 |\n",
            "|    n_updates        | 23870    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15928    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95606    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2e-06  |\n",
            "|    n_updates        | 23876    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15932    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95630    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46e-06 |\n",
            "|    n_updates        | 23882    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15936    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95654    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.79e-07 |\n",
            "|    n_updates        | 23888    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15940    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95678    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12e-06 |\n",
            "|    n_updates        | 23894    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15944    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95702    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.15e-05 |\n",
            "|    n_updates        | 23900    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15948    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95726    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.52e-05 |\n",
            "|    n_updates        | 23906    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15952    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95750    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0522   |\n",
            "|    n_updates        | 23912    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15956    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95774    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.86e-05 |\n",
            "|    n_updates        | 23918    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15960    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95798    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.56e-05 |\n",
            "|    n_updates        | 23924    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15964    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95822    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.97e-06 |\n",
            "|    n_updates        | 23930    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15968    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95846    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.96e-06 |\n",
            "|    n_updates        | 23936    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15972    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95870    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.98e-06 |\n",
            "|    n_updates        | 23942    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15976    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95894    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.39e-06 |\n",
            "|    n_updates        | 23948    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15980    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95918    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.67e-06 |\n",
            "|    n_updates        | 23954    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15984    |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 95942    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-06 |\n",
            "|    n_updates        | 23960    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15988    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 95966    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.25e-06 |\n",
            "|    n_updates        | 23966    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15992    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 95990    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05e-06 |\n",
            "|    n_updates        | 23972    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 15996    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96014    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06e-06 |\n",
            "|    n_updates        | 23978    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16000    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96038    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.13e-07 |\n",
            "|    n_updates        | 23984    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16004    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96062    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.94e-06 |\n",
            "|    n_updates        | 23990    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16008    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96086    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.38e-05 |\n",
            "|    n_updates        | 23996    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16012    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96110    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.26e-05 |\n",
            "|    n_updates        | 24002    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16016    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96134    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7e-06    |\n",
            "|    n_updates        | 24008    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16020    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96158    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.16e-06 |\n",
            "|    n_updates        | 24014    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16024    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96182    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03e-06 |\n",
            "|    n_updates        | 24020    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16028    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96206    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.2e-06  |\n",
            "|    n_updates        | 24026    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16032    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96230    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.23e-06 |\n",
            "|    n_updates        | 24032    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16036    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96254    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.43e-07 |\n",
            "|    n_updates        | 24038    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16040    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96278    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25e-06 |\n",
            "|    n_updates        | 24044    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16044    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96302    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.68e-06 |\n",
            "|    n_updates        | 24050    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16048    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96326    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.36e-07 |\n",
            "|    n_updates        | 24056    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16052    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96350    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76e-06 |\n",
            "|    n_updates        | 24062    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16056    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96374    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.54e-07 |\n",
            "|    n_updates        | 24068    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -2.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16060    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96398    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.34e-06 |\n",
            "|    n_updates        | 24074    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16064    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96422    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24e-05 |\n",
            "|    n_updates        | 24080    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16068    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 96446    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.62e-05 |\n",
            "|    n_updates        | 24086    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16072    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96470    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.95e-05 |\n",
            "|    n_updates        | 24092    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16076    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96494    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.01e-05 |\n",
            "|    n_updates        | 24098    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16080    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96518    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17e-05 |\n",
            "|    n_updates        | 24104    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16084    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96542    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.38e-05 |\n",
            "|    n_updates        | 24110    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16088    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96566    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04e-05 |\n",
            "|    n_updates        | 24116    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16092    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96590    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.49e-06 |\n",
            "|    n_updates        | 24122    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16096    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96614    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.72e-06 |\n",
            "|    n_updates        | 24128    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16100    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96638    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.74e-06 |\n",
            "|    n_updates        | 24134    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16104    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96662    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.83e-07 |\n",
            "|    n_updates        | 24140    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16108    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96686    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.98e-06 |\n",
            "|    n_updates        | 24146    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16112    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96710    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.85e-06 |\n",
            "|    n_updates        | 24152    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16116    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96734    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.93e-06 |\n",
            "|    n_updates        | 24158    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16120    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96758    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.3e-06  |\n",
            "|    n_updates        | 24164    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16124    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96782    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.59e-06 |\n",
            "|    n_updates        | 24170    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16128    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96806    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.88e-06 |\n",
            "|    n_updates        | 24176    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.2      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16132    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96830    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.03e-07 |\n",
            "|    n_updates        | 24182    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16136    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96854    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.54e-06 |\n",
            "|    n_updates        | 24188    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16140    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96878    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.35e-06 |\n",
            "|    n_updates        | 24194    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16144    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96902    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23e-06 |\n",
            "|    n_updates        | 24200    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16148    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96926    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17e-06 |\n",
            "|    n_updates        | 24206    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16152    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96950    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.13e-06 |\n",
            "|    n_updates        | 24212    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16156    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96974    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14e-06 |\n",
            "|    n_updates        | 24218    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16160    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 96998    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27e-06 |\n",
            "|    n_updates        | 24224    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16164    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 97022    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.83e-06 |\n",
            "|    n_updates        | 24230    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16168    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 97046    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.96e-06 |\n",
            "|    n_updates        | 24236    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16172    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97070    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.29e-05 |\n",
            "|    n_updates        | 24242    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16176    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97094    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.01e-05 |\n",
            "|    n_updates        | 24248    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.32     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16180    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97118    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.16e-06 |\n",
            "|    n_updates        | 24254    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16184    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97142    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.56e-06 |\n",
            "|    n_updates        | 24260    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16188    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97166    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.8e-06  |\n",
            "|    n_updates        | 24266    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16192    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97190    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07e-06 |\n",
            "|    n_updates        | 24272    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16196    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97214    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.3e-06  |\n",
            "|    n_updates        | 24278    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16200    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97238    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21e-06 |\n",
            "|    n_updates        | 24284    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16204    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97262    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.91e-06 |\n",
            "|    n_updates        | 24290    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16208    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97286    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2e-06  |\n",
            "|    n_updates        | 24296    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16212    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97310    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82e-06 |\n",
            "|    n_updates        | 24302    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -3.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16216    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97334    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.79e-06 |\n",
            "|    n_updates        | 24308    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16220    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97358    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.8e-07  |\n",
            "|    n_updates        | 24314    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16224    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97382    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.88e-06 |\n",
            "|    n_updates        | 24320    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16228    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97406    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.4e-06  |\n",
            "|    n_updates        | 24326    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16232    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97430    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.16e-06 |\n",
            "|    n_updates        | 24332    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16236    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97454    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.33e-06 |\n",
            "|    n_updates        | 24338    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16240    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97478    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46e-06 |\n",
            "|    n_updates        | 24344    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16244    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97502    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5e-06  |\n",
            "|    n_updates        | 24350    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16248    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97526    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.49e-07 |\n",
            "|    n_updates        | 24356    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16252    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97550    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.94e-06 |\n",
            "|    n_updates        | 24362    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16256    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97574    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05e-06 |\n",
            "|    n_updates        | 24368    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16260    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97598    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.57e-07 |\n",
            "|    n_updates        | 24374    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16264    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97622    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.93e-05 |\n",
            "|    n_updates        | 24380    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16268    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 97646    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.89e-05 |\n",
            "|    n_updates        | 24386    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16272    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97670    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-05 |\n",
            "|    n_updates        | 24392    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16276    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97694    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.46e-06 |\n",
            "|    n_updates        | 24398    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16280    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97718    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.31e-06 |\n",
            "|    n_updates        | 24404    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16284    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97742    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.25e-06 |\n",
            "|    n_updates        | 24410    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16288    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97766    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18e-06 |\n",
            "|    n_updates        | 24416    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16292    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97790    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.81e-05 |\n",
            "|    n_updates        | 24422    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16296    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97814    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000119 |\n",
            "|    n_updates        | 24428    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16300    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97838    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.6e-05  |\n",
            "|    n_updates        | 24434    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16304    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97862    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5e-05  |\n",
            "|    n_updates        | 24440    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16308    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97886    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.53e-05 |\n",
            "|    n_updates        | 24446    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16312    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97910    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.86e-06 |\n",
            "|    n_updates        | 24452    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16316    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97934    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.27e-06 |\n",
            "|    n_updates        | 24458    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16320    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97958    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.29e-06 |\n",
            "|    n_updates        | 24464    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16324    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 97982    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.37e-06 |\n",
            "|    n_updates        | 24470    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16328    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98006    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0522   |\n",
            "|    n_updates        | 24476    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16332    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98030    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.32e-05 |\n",
            "|    n_updates        | 24482    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16336    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98054    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41e-05 |\n",
            "|    n_updates        | 24488    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16340    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98078    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.8e-06  |\n",
            "|    n_updates        | 24494    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16344    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98102    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.2e-06  |\n",
            "|    n_updates        | 24500    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16348    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98126    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.28e-06 |\n",
            "|    n_updates        | 24506    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16352    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98150    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.04e-05 |\n",
            "|    n_updates        | 24512    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16356    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98174    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.4e-05  |\n",
            "|    n_updates        | 24518    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16360    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98198    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.92e-06 |\n",
            "|    n_updates        | 24524    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16364    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98222    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.32e-06 |\n",
            "|    n_updates        | 24530    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16368    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 98246    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.73e-06 |\n",
            "|    n_updates        | 24536    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16372    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98270    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41e-06 |\n",
            "|    n_updates        | 24542    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16376    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98294    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5e-06  |\n",
            "|    n_updates        | 24548    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16380    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98318    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.34e-06 |\n",
            "|    n_updates        | 24554    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16384    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98342    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.01e-06 |\n",
            "|    n_updates        | 24560    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16388    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98366    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04e-06 |\n",
            "|    n_updates        | 24566    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16392    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98390    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.16e-06 |\n",
            "|    n_updates        | 24572    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16396    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98414    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.01e-07 |\n",
            "|    n_updates        | 24578    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.3      |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16400    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98438    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.38e-07 |\n",
            "|    n_updates        | 24584    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.27     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16404    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98462    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.46e-07 |\n",
            "|    n_updates        | 24590    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16408    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98486    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.48e-07 |\n",
            "|    n_updates        | 24596    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16412    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98510    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.42e-07 |\n",
            "|    n_updates        | 24602    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16416    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98534    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21e-06 |\n",
            "|    n_updates        | 24608    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16420    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98558    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.51e-07 |\n",
            "|    n_updates        | 24614    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16424    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98582    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.51e-06 |\n",
            "|    n_updates        | 24620    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16428    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98606    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-06 |\n",
            "|    n_updates        | 24626    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16432    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98630    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.18e-07 |\n",
            "|    n_updates        | 24632    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16436    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98654    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.64e-05 |\n",
            "|    n_updates        | 24638    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16440    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98678    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.93e-05 |\n",
            "|    n_updates        | 24644    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16444    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98702    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05e-05 |\n",
            "|    n_updates        | 24650    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16448    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98726    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.97e-06 |\n",
            "|    n_updates        | 24656    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16452    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98750    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.51e-06 |\n",
            "|    n_updates        | 24662    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16456    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98774    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.4e-06  |\n",
            "|    n_updates        | 24668    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16460    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98798    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.69e-06 |\n",
            "|    n_updates        | 24674    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.29     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16464    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98822    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57e-06 |\n",
            "|    n_updates        | 24680    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.31     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16468    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 98846    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.99e-07 |\n",
            "|    n_updates        | 24686    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16472    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98870    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.87e-06 |\n",
            "|    n_updates        | 24692    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16476    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98894    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.6e-06  |\n",
            "|    n_updates        | 24698    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16480    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98918    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.47e-07 |\n",
            "|    n_updates        | 24704    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16484    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98942    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.33e-07 |\n",
            "|    n_updates        | 24710    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16488    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98966    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.47e-06 |\n",
            "|    n_updates        | 24716    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16492    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 98990    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09e-06 |\n",
            "|    n_updates        | 24722    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16496    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99014    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9e-08    |\n",
            "|    n_updates        | 24728    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.22     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16500    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99038    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.52e-06 |\n",
            "|    n_updates        | 24734    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.23     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16504    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99062    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04e-05 |\n",
            "|    n_updates        | 24740    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16508    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99086    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2e-05    |\n",
            "|    n_updates        | 24746    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.26     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16512    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99110    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.13e-06 |\n",
            "|    n_updates        | 24752    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16516    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99134    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.73e-05 |\n",
            "|    n_updates        | 24758    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.24     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16520    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99158    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39e-05 |\n",
            "|    n_updates        | 24764    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16524    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99182    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2e-05  |\n",
            "|    n_updates        | 24770    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16528    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99206    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.87e-06 |\n",
            "|    n_updates        | 24776    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16532    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99230    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.03e-06 |\n",
            "|    n_updates        | 24782    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16536    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99254    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.3e-06  |\n",
            "|    n_updates        | 24788    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16540    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99278    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.63e-06 |\n",
            "|    n_updates        | 24794    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16544    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99302    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.18e-07 |\n",
            "|    n_updates        | 24800    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16548    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99326    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04e-06 |\n",
            "|    n_updates        | 24806    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16552    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99350    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.97e-07 |\n",
            "|    n_updates        | 24812    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16556    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99374    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.15e-07 |\n",
            "|    n_updates        | 24818    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16560    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99398    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57e-06 |\n",
            "|    n_updates        | 24824    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16564    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99422    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.06e-07 |\n",
            "|    n_updates        | 24830    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.13     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16568    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 99446    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48e-06 |\n",
            "|    n_updates        | 24836    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16572    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99470    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27e-06 |\n",
            "|    n_updates        | 24842    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16576    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99494    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.64e-07 |\n",
            "|    n_updates        | 24848    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16580    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99518    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06e-06 |\n",
            "|    n_updates        | 24854    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16584    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99542    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.07e-07 |\n",
            "|    n_updates        | 24860    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16588    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99566    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.98e-06 |\n",
            "|    n_updates        | 24866    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.14     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16592    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99590    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.29e-07 |\n",
            "|    n_updates        | 24872    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.14     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16596    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99614    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.98e-07 |\n",
            "|    n_updates        | 24878    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -1.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.11     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16600    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99638    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.93e-07 |\n",
            "|    n_updates        | 24884    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16604    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99662    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.16e-07 |\n",
            "|    n_updates        | 24890    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16608    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99686    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13e-06 |\n",
            "|    n_updates        | 24896    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.12     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16612    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99710    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24e-06 |\n",
            "|    n_updates        | 24902    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.14     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16616    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99734    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.65e-07 |\n",
            "|    n_updates        | 24908    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 0.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.13     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16620    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99758    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.6e-05  |\n",
            "|    n_updates        | 24914    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16624    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99782    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.09e-05 |\n",
            "|    n_updates        | 24920    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16628    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99806    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.06e-06 |\n",
            "|    n_updates        | 24926    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.16     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16632    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99830    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.14e-06 |\n",
            "|    n_updates        | 24932    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16636    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99854    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.44e-06 |\n",
            "|    n_updates        | 24938    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16640    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99878    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.36e-06 |\n",
            "|    n_updates        | 24944    |\n",
            "----------------------------------\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.17     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16644    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99902    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.52e-07 |\n",
            "|    n_updates        | 24950    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.19     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16648    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99926    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.35e-06 |\n",
            "|    n_updates        | 24956    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.21     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16652    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99950    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32e-06 |\n",
            "|    n_updates        | 24962    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: -0.6\n",
            "Episode Reward: 0.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16656    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99974    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.06e-06 |\n",
            "|    n_updates        | 24968    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "Episode Reward: 1.4\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 6        |\n",
            "|    ep_rew_mean      | 1.18     |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16660    |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 99998    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.21e-07 |\n",
            "|    n_updates        | 24974    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=1.40 +/- 0.00\n",
            "Episode length: 6.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 6        |\n",
            "|    mean_reward      | 1.4      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.02     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "----------------------------------\n",
            "Training complete. Models saved to: models/dqn/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from custom_env import EducationHubEnv  # Your fixed custom env\n",
        "\n",
        "# Custom callback to log rewards\n",
        "class EducationLoggerCallback(BaseCallback):\n",
        "    def __init__(self, log_dir, verbose=1):\n",
        "        super(EducationLoggerCallback, self).__init__(verbose)\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        self.episode_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if \"infos\" in self.locals and len(self.locals[\"infos\"]) > 0:\n",
        "            episode_reward = self.locals[\"infos\"][0].get(\"episode\", {}).get(\"r\")\n",
        "            if episode_reward is not None:\n",
        "                self.episode_rewards.append(episode_reward)\n",
        "                print(f\"Episode Reward: {episode_reward}\")\n",
        "        return True\n",
        "\n",
        "    def on_training_end(self):\n",
        "        np.save(os.path.join(self.log_dir, \"dqn_rewards.npy\"), np.array(self.episode_rewards))\n",
        "\n",
        "def train_dqn():\n",
        "    # ✅ Wrap environment properly for SB3\n",
        "    env = DummyVecEnv([lambda: Monitor(EducationHubEnv())])\n",
        "\n",
        "    # Define paths\n",
        "    models_dir = \"models/dqn/\"\n",
        "    log_dir = \"logs/dqn/\"\n",
        "    best_model_dir = os.path.join(models_dir, \"best/\")\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    os.makedirs(best_model_dir, exist_ok=True)\n",
        "\n",
        "    # ✅ DQN Model with tuned hyperparameters\n",
        "    model = DQN(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        learning_rate=1e-4,\n",
        "        gamma=0.99,\n",
        "        batch_size=64,\n",
        "        buffer_size=100000,\n",
        "        exploration_initial_eps=1.0,\n",
        "        exploration_final_eps=0.02,\n",
        "        exploration_fraction=0.2,\n",
        "        tensorboard_log=log_dir\n",
        "    )\n",
        "\n",
        "    # ✅ Configure logger (stdout + tensorboard)\n",
        "    logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
        "    model.set_logger(logger)\n",
        "\n",
        "    # ✅ Callbacks\n",
        "    reward_logger = EducationLoggerCallback(log_dir)\n",
        "    eval_callback = EvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=best_model_dir,\n",
        "        eval_freq=5000,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    # ✅ Train!\n",
        "    model.learn(\n",
        "        total_timesteps=100000,\n",
        "        callback=[reward_logger, eval_callback]\n",
        "    )\n",
        "\n",
        "    # ✅ Save final model\n",
        "    model.save(os.path.join(models_dir, \"dqn_final_model\"))\n",
        "    print(\"Training complete. Models saved to:\", models_dir)\n",
        "\n",
        "    # ✅ Close environment\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dqn()"
      ]
    }
  ]
}