# -*- coding: utf-8 -*-
"""DQN-Training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17xp8LxLC0NZ85PTMGFq57CrbfnRPIrSY
"""

!pip install stable-baselines3

import sys
import os

# Add the directory where custom_env.py is located
sys.path.append('/content/')

import os
import numpy as np
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.logger import configure
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from custom_env import EducationHubEnv  # Your fixed custom env

# Custom callback to log rewards
class EducationLoggerCallback(BaseCallback):
    def __init__(self, log_dir, verbose=1):
        super(EducationLoggerCallback, self).__init__(verbose)
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        self.episode_rewards = []

    def _on_step(self) -> bool:
        if "infos" in self.locals and len(self.locals["infos"]) > 0:
            episode_reward = self.locals["infos"][0].get("episode", {}).get("r")
            if episode_reward is not None:
                self.episode_rewards.append(episode_reward)
                print(f"Episode Reward: {episode_reward}")
        return True

    def on_training_end(self):
        np.save(os.path.join(self.log_dir, "dqn_rewards.npy"), np.array(self.episode_rewards))

def train_dqn():
    # ✅ Wrap environment properly for SB3
    env = DummyVecEnv([lambda: Monitor(EducationHubEnv())])

    # Define paths
    models_dir = "models/dqn/"
    log_dir = "logs/dqn/"
    best_model_dir = os.path.join(models_dir, "best/")
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(best_model_dir, exist_ok=True)

    # ✅ DQN Model with tuned hyperparameters
    model = DQN(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=1e-4,
        gamma=0.99,
        batch_size=64,
        buffer_size=100000,
        exploration_initial_eps=1.0,
        exploration_final_eps=0.02,
        exploration_fraction=0.2,
        tensorboard_log=log_dir
    )

    # ✅ Configure logger (stdout + tensorboard)
    logger = configure(log_dir, ["stdout", "tensorboard"])
    model.set_logger(logger)

    # ✅ Callbacks
    reward_logger = EducationLoggerCallback(log_dir)
    eval_callback = EvalCallback(
        env,
        best_model_save_path=best_model_dir,
        eval_freq=5000,
        deterministic=True,
        render=False
    )

    # ✅ Train!
    model.learn(
        total_timesteps=100000,
        callback=[reward_logger, eval_callback]
    )

    # ✅ Save final model
    model.save(os.path.join(models_dir, "dqn_final_model"))
    print("Training complete. Models saved to:", models_dir)

    # ✅ Close environment
    env.close()

if __name__ == "__main__":
    train_dqn()